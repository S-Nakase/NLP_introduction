{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文書分類とその入力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 文書分類：「文書」を何らかのカテゴリーに分類するタスクのこと\n",
    ">- スパムメールであるか否かの判別、評判分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私\t名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "秋田\t名詞,固有名詞,地域,一般,*,*,秋田,アキタ,アキタ\n",
      "犬\t名詞,一般,*,*,*,*,犬,イヌ,イヌ\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "大好き\t名詞,形容動詞語幹,*,*,*,*,大好き,ダイスキ,ダイスキ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n"
     ]
    }
   ],
   "source": [
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()  # 形態素解析器をtとする\n",
    "for token in t.tokenize('私は秋田犬が大好き。'):  # 解析結果を1単語ずつtokenに保存していく \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私/は/秋田/犬/が/大好き/。/"
     ]
    }
   ],
   "source": [
    "for token in t.tokenize('私は秋田犬が大好き。', wakati=True):  # wakati=Trueで単語分割を目的とした分かち書きを行う\n",
    "    print(token, end='/')  # end='/'で、単語ごとに「/」で区切る指定を加える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token for token in t.tokenize('私は秋田犬が大好き。', wakati=True)]  # 単語分割の結果をリスト型にして取っておく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['私', 'は', '秋田', '犬', 'が', '大好き', '。']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words  #uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['私', 'は']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['は', '秋田']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['秋田', '犬']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gramを得るには、左側のインデックスにnを足した数を右側のインデックスに指定する必要がある\n",
    "# bi-gramなら、[1:3],[2:4]など、左の数字に2を足すと右の数字になる\n",
    "\n",
    "def get_word_n_grams(sentence, n):  # 引数をsentence(解析を行う文章)とn(N-gramのnの値)とするget_word_n_grams関数\n",
    "    words = [token for token in t.tokenize(sentence, wakati=True)]  # 単語分割の結果をリスト型にして取ったもの\n",
    "    result = []  # 結果を保存するリスト\n",
    "    for index in range(len(words)):  # indexには0,1,2,...の順番に値が入る\n",
    "        result.append(words[index: index+n])\n",
    "        if index + n >= len(words):  # 指定しなければN-gramを算出できない状況が発生\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['私'], ['は'], ['秋田'], ['犬'], ['が'], ['大好き'], ['。']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '私は秋田犬が大好き。'\n",
    "get_word_n_grams(sentence, 1)  # 引数が1→uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['私', 'は'], ['は', '秋田'], ['秋田', '犬'], ['犬', 'が'], ['が', '大好き'], ['大好き', '。']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_n_grams(sentence, 2)  #bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['私', 'は', '秋田'],\n",
       " ['は', '秋田', '犬'],\n",
       " ['秋田', '犬', 'が'],\n",
       " ['犬', 'が', '大好き'],\n",
       " ['が', '大好き', '。']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_n_grams(sentence, 3)  #tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字のN-gram\n",
    "def get_character_n_grams(sentence, n):\n",
    "    result = []\n",
    "    for index in range(len(sentence)):  # 文字単位→文章を直接入れるだけ\n",
    "        result.append(sentence[index: index+n])\n",
    "        if index + n >= len(sentence):\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['私', 'は', '秋', '田', '犬', 'が', '大', '好', 'き', '。']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_character_n_grams(sentence, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['私は', 'は秋', '秋田', '田犬', '犬が', 'が大', '大好', '好き', 'き。']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_character_n_grams(sentence, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['私は秋', 'は秋田', '秋田犬', '田犬が', '犬が大', 'が大好', '大好き', '好き。']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_character_n_grams(sentence, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 単語の出現する順番は考えない\n",
    ">- 「私は秋田犬が大好き。」と「秋田犬は私が大好き。」は同じ表現となる\n",
    ">- 「犬に関する文書か否か」を分類するだけならBOWで分類可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['私'], ['は'], ['犬'], ['が'], ['少し'], ['苦手'], ['。']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = '私は犬が少し苦手。'\n",
    "get_word_n_grams(sentence2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['秋田'], ['犬'], ['は'], ['私'], ['が'], ['大好き'], ['。']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence3 = '秋田犬は私が大好き。'\n",
    "get_word_n_grams(sentence3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words(document):  # 文書document中の単語の出現回数を数える関数\n",
    "    result_dict = {}  # 辞書\n",
    "    words=[token for token in t.tokenize(document, wakati=True)]  # 単語分割の結果リスト\n",
    "    for word in words:\n",
    "        if word not in result_dict:  # 語wordが辞書になければ\n",
    "          result_dict[word]=1        # 出現回数を1とする\n",
    "        else:                        # 辞書にあれば\n",
    "          result_dict[word] += 1     # これまでの出現回数に１を加える\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = '私は秋田犬が大好き。秋田犬は私が大好き。'  # 2つの文章は、BOW上では同じ表現である\n",
    "document2 = '私は犬が少し苦手。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'私': 2, 'は': 2, '秋田': 2, '犬': 2, 'が': 2, '大好き': 2, '。': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bag_of_words(document1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'私': 1, 'は': 1, '犬': 1, 'が': 1, '少し': 1, '苦手': 1, '。': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bag_of_words(document2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 用例ベクトル：文書や文一つ一つの用例を、それぞれ一つのベクトルとして表したもの\n",
    ">-  ベクトル空間モデル：様々な言語的アイテムをベクトルとして表す数理モデルのこと\n",
    ">- ある単語をある数値と紐づけて表現する→名義尺度\n",
    ">- 単語や品詞など、カテゴリが決まっているだけのデータを変数にする→ダミー変数\n",
    "\n",
    ">- BOWにおいては、「用例中にある単語が出てくる出現回数」をベクトルとする\n",
    ">- 「私」をx1、「は」をx2、...などとし、コーパス中の語彙サイズをNとするとxNまでの変数が必要となる\n",
    ">- 素性：x1,x2等の変数、素性値：その変数の値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(documents):  # 素性の辞書を作成する関数\n",
    "    result_dict = {}\n",
    "    index=1\n",
    "    for adocument in documents:\n",
    "      words=[token for token in t.tokenize(adocument, wakati=True)]\n",
    "      for word in words:\n",
    "        if word not in result_dict:\n",
    "          result_dict[word]=index\n",
    "          index+=1\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'私': 1, 'は': 2, '秋田': 3, '犬': 4, 'が': 5, '大好き': 6, '。': 7, '少し': 8, '苦手': 9}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "document1 = '私は秋田犬が大好き。秋田犬は私が大好き。'\n",
    "document2 = '私は犬が少し苦手。'\n",
    "documents = [document1, document2]\n",
    "dict = make_dictionary(documents)\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 次元の高い(語彙サイズの大きい)文書の場合、用例ベクトルがほとんど0となる→スパース(疎)なベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_BOW_vectors(documents, dictionary):  # 用例ベクトルを作成する関数、辞書にある単語の出現回数\n",
    "  result_vectors=[]\n",
    "  for adocument in documents:\n",
    "    avector={}\n",
    "    words=[token for token in t.tokenize(adocument, wakati=True)]\n",
    "    for entry in dictionary:\n",
    "      avector[dictionary[entry]]=0\n",
    "    for word in words:\n",
    "      avector[dictionary[word]]+=1\n",
    "    result_vectors.append(avector)\n",
    "  return result_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 2, 2: 2, 3: 2, 4: 2, 5: 2, 6: 2, 7: 2, 8: 0, 9: 0},\n",
       " {1: 1, 2: 1, 3: 0, 4: 1, 5: 1, 6: 0, 7: 1, 8: 1, 9: 1}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_BOW_vectors(documents, dict)  # 1~9はmake_dictionary関数で作成した素性の辞書における単語に対応している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 「uni-gramのBag-of-wordsの出現」を素性とする方法→コーパスサイズを考慮したN-gramが実現\n",
    ">- 対応する単語が出現すれば1、出現しなければ0と素性値を指定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_BOW_onehot_vectors(documents, dictionary):  #「素性の辞書にあれば1、なければ0を返す」\n",
    "  result_vectors=[]\n",
    "  for adocument in documents:\n",
    "    avector={}\n",
    "    words=[token for token in t.tokenize(adocument, wakati=True)]\n",
    "    for entry in dictionary:\n",
    "      avector[dictionary[entry]]=0\n",
    "    for word in words:\n",
    "      avector[dictionary[word]]=1  # ここがさっきと異なる\n",
    "    result_vectors.append(avector)\n",
    "  return result_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0},\n",
       " {1: 1, 2: 1, 3: 0, 4: 1, 5: 1, 6: 0, 7: 1, 8: 1, 9: 1}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_BOW_onehot_vectors(documents, dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- CountVectorizerモジュールによる単語の出現頻度の文書ベクトル\n",
    ">- CountVectorizer{sklearn}では、コーパスからBOWの出現回数の行列に変換が可能\n",
    ">- 用例数×素性数の行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus(documents):  # 文書からコーパスを作成する関数\n",
    "  result_corpus=[]\n",
    "  for adocument in documents:\n",
    "    words=[token for token in t.tokenize(adocument, wakati=True)]\n",
    "    text=\" \".join(words)  # join関数によって、文字列を結合する\n",
    "    result_corpus.append(text)\n",
    "  return result_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['私 は 秋田 犬 が 大好き 。 秋田 犬 は 私 が 大好き 。', '私 は 犬 が 少し 苦手 。']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "document1 = '私は秋田犬が大好き。秋田犬は私が大好き。'\n",
    "document2 = '私は犬が少し苦手。'\n",
    "documents = [document1, document2]\n",
    "corpus = make_corpus(documents)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['が' 'が 大好き' 'が 少し' 'は' 'は 犬' 'は 私' 'は 秋田' '大好き' '大好き 秋田' '少し' '少し 苦手' '犬'\n",
      " '犬 が' '犬 は' '私' '私 が' '私 は' '秋田' '秋田 犬' '苦手']\n",
      "[[2 2 0 2 0 1 1 2 1 0 0 2 1 1 2 1 1 2 2 0]\n",
      " [1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', ngram_range=(1,2))  \n",
    "#一文字の単語を許容するように、token_patternを指定する\n",
    "# ngram_rangeのベクトルで、nの値を指定(今回はuni-gramとbi-gramの出現回数の行列を表示するように指定)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out(corpus))  # dictに対応\n",
    "print(X.toarray())  # 疎な行列Xを0が見える形の行列形式に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF：単語の重要度を考慮した値\n",
    "* 参考：https://dev.classmethod.jp/articles/yoshim_2017ad_tfidf_1-2/\n",
    "* 文書を分類するために重要な単語と重要でない単語が存在する\n",
    "* たとえば、音楽に関する文書を分類する際に、楽器の名前は重要だが、食べ物の名前は重要でない\n",
    "* 重要な単語は、その文書内における出現頻度が高い、かつ、他の文書内にはあまり出現しない\n",
    "* TF：単語の出現頻度(Term Frequency)、IDF：単語の逆文書頻度(Inverse Document Frequency)\n",
    "\n",
    "* $ 単語の出現回数：tf_{i,j} = n_{i,j} \\\\ 単語の出現比率：tf_{i,j} = \\frac{n_{i,j}}{\\sum_k{n_{i,j}}} \\\\ 逆文書頻度：idf_{i,j} = \\log{\\frac{|D|}{|d:w_i \\in d|}} \\\\ d：単語w_iを含む文書数 \\\\ |D|：コーパス内の総文書数 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['が' 'が 大好き' 'が 少し' 'は' 'は 犬' 'は 私' 'は 秋田' '大好き' '大好き 秋田' '少し' '少し 苦手' '犬'\n",
      " '犬 が' '犬 は' '私' '私 が' '私 は' '秋田' '秋田 犬' '苦手']\n",
      "[[0.25932077 0.36446629 0.         0.25932077 0.         0.18223315\n",
      "  0.18223315 0.36446629 0.18223315 0.         0.         0.25932077\n",
      "  0.12966039 0.18223315 0.25932077 0.18223315 0.12966039 0.36446629\n",
      "  0.36446629 0.        ]\n",
      " [0.25096919 0.         0.35272845 0.25096919 0.35272845 0.\n",
      "  0.         0.         0.         0.35272845 0.35272845 0.25096919\n",
      "  0.25096919 0.         0.25096919 0.         0.25096919 0.\n",
      "  0.         0.35272845]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out(corpus))\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA(Latent Semantic Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- LSA(潜在意味解析)：疎な行列である用例ベクトルの行列を、低次元の意味空間に落とし込むことで、密な行列にするという手法\n",
    ">- 主成分分析に近い手法\n",
    ">- 語彙サイズの考慮、類義語・多義語の考慮\n",
    ">- 特異値分解によって、素性数×文書数の行列 = 単語数×トピック数の行列 + トピック数分の特異値 + トピック数×文書数の行列とする\n",
    ">- 特異値の順番で、上位r個のトピックを選び、任意の要素に次元削減可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私 は 秋田 犬 が 大好き 。 秋田 犬 は 私 が 大好き 。', '私 は 犬 が 少し 苦手 。', '私 は ぶた と ペンギン が 好き です 。 ペンギン は 鳥 の 仲間 です が 、 水族館 で 見る こと が でき ます 。', '隣 の 家 の 犬 は よく 吠える 犬 です 。', 'ゴロピカドン が 好き です 。', '事務 室 の カギ が 見つから ない と 思っ た が 、 よく 探し たら あっ た 。', 'フルーツ は どれ も 大好き です が 、 私 の 好物 は イチゴ です 。', 'りんご も ミカン も パイナップル も 、 どれ も それぞれ とても 美味しい 果物 です 。', '私 は フルーツ が 好き で 、 よく フルーツパーラー に 食べ に 行き ます 。', 'ぶどう と メロン は 必需 品 。']\n"
     ]
    }
   ],
   "source": [
    "document1='私は秋田犬が大好き。秋田犬は私が大好き。'\n",
    "document2='私は犬が少し苦手。'\n",
    "document3='私はぶたとペンギンが好きです。ペンギンは鳥の仲間ですが、水族館で見ることができます。'\n",
    "document4='隣の家の犬はよく吠える犬です。'\n",
    "document5='ゴロピカドンが好きです。'\n",
    "document6='事務室のカギが見つからないと思ったが、よく探したらあった。'\n",
    "document7='フルーツはどれも大好きですが、私の好物はイチゴです。'\n",
    "document8='りんごもミカンもパイナップルも、どれもそれぞれとても美味しい果物です。'\n",
    "document9='私はフルーツが好きで、よくフルーツパーラーに食べに行きます。'\n",
    "document10='ぶどうとメロンは必需品。'\n",
    "documents=[document1, document2,document3, document4, document5, document6, document7, document8, document9, document10]\n",
    "\n",
    "corpus=make_corpus(documents)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 57)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', sublinear_tf=True, use_idf=True)\n",
    " # sublinear_tf=False, use_idf=Trueがデフォルト　use_idf=Falseにすると、どの単語もIDF=1となる\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape) #文書やrandom_stateのシードの数によって結果が異なります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63766684 -0.41904035  0.23741743 -0.10413639 -0.08510271  0.03668821\n",
      "  -0.247584  ]\n",
      " [ 0.56856989 -0.4931894   0.15201046 -0.1299718  -0.07666711 -0.01388097\n",
      "  -0.27937207]\n",
      " [ 0.58417091  0.22669084 -0.37712668 -0.0455379  -0.12673344 -0.13179484\n",
      "   0.08388958]\n",
      " [ 0.51624345 -0.25649865  0.13492393  0.1127026   0.34340273 -0.23962655\n",
      "   0.66353769]\n",
      " [ 0.42976803  0.42018574 -0.31602309 -0.27390951  0.02095586 -0.52088956\n",
      "  -0.19645644]\n",
      " [ 0.25962181 -0.00758344 -0.35962843  0.48400399  0.66850129  0.17891578\n",
      "  -0.29967936]\n",
      " [ 0.67412904  0.28524     0.34269431  0.06485862 -0.03318881  0.08831955\n",
      "  -0.0092058 ]\n",
      " [ 0.21590475  0.61211124  0.58765645  0.21852602  0.02763261  0.12820898\n",
      "  -0.05333523]\n",
      " [ 0.45732415  0.13258818 -0.39092101 -0.26445099 -0.15173526  0.63941905\n",
      "   0.21545951]\n",
      " [ 0.17729441 -0.09072649 -0.20928661  0.73785634 -0.5761692  -0.09609122\n",
      "   0.03714835]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=7, n_iter=5, random_state=42)  \n",
    "  # ncomponents=7：上位7つの主要な要素に次元削減、n_iter=5：5回繰り返す、random_state=42：set.seed(42)と同じ\n",
    "newX = svd.fit_transform(X)\n",
    "print(newX) #文書やrandom_stateのシードの数によって結果が異なります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7)\n"
     ]
    }
   ],
   "source": [
    "print(newX.shape)  # 語彙サイズ57のうち、50は疎なベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03667877, 0.15012475, 0.14335443, 0.12162776, 0.12126485,\n",
       "       0.10472207, 0.09752666])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 寄与率\n",
    "svd.explained_variance_ratio_ #文書やrandom_stateのシードの数によって結果が異なります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77529930095747"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 累積寄与率\n",
    "svd.explained_variance_ratio_.sum()  \n",
    "# 次元数と累積寄与率はトレードオフ、次元数を増やせば累積寄与率は高くなる、調整が必要"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
