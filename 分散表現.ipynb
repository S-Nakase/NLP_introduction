{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分散表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 分散表現：疎な分布のベクトルから、低次元で実数のベクトル空間に数学的な「埋め込み」を行って作成した、コンピュータで扱いやすい密なベクトル\n",
    "* LSA → 「同じ文書に出てくる単語の出現回数」の分布を数学的に処理して「意味」を扱う\n",
    "* SVD、確率モデル、ニューラルネットワークなどを用いる\n",
    "* 「私は私。」は、「私」「は」「私」「。」という４つのトークンからなり、「私」「は」「。」という３つのタイプから構成されている\n",
    "* 語彙レベルの分散表現(word2vec、fastText、Gloveなど)は、単語には固有の意味があると考える→類義語には対応可だが、多義語には対応不可\n",
    "\n",
    "* ディープラーニング→出現レベルの単語の分散表現(単語の出現する文書ごとにその単語のベクトルが異なるため、多義語にも対応可)\n",
    "* EIMo、BERTなど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cos類似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 単語の意味的な類似性を求める\n",
    "* 疎なベクトル同士の計算にも適用可能\n",
    "\n",
    "$ \\cos(\\overrightarrow{A},\\overrightarrow{B}) = \\frac{\\sum_{n=1}^{i}a_i b_i}{\\sqrt{\\sum_{n=1}^{i}a_i^2} \\sqrt{\\sum_{n=1}^{i}b_i^2}} $\n",
    "\n",
    "* AとBは文書である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要データ\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "document1='私は秋田犬が大好き。秋田犬は私が大好き。'\n",
    "document2='私は犬が少し苦手。'\n",
    "document3='私はぶたとペンギンが好きです。ペンギンは鳥の仲間ですが、水族館で見ることができます。'\n",
    "document4='隣の家の犬はよく吠える犬です。'\n",
    "document5='ゴロピカドンが好きです。'\n",
    "document6='事務室のカギが見つからないと思ったが、よく探したらあった。'\n",
    "document7='フルーツはどれも大好きですが、私の好物はイチゴです。'\n",
    "document8='りんごもミカンもパイナップルも、どれもそれぞれとても美味しい果物です。'\n",
    "document9='私はフルーツが好きで、よくフルーツパーラーに食べに行きます。'\n",
    "document10='ぶどうとメロンは必需品。'\n",
    "documents=[document1, document2,document3, document4, document5, document6, document7, document8, document9, document10]\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "def make_corpus(documents):\n",
    "  result_corpus=[]\n",
    "  for adocument in documents:\n",
    "    words=[token for token in t.tokenize(adocument, wakati=True)]\n",
    "    text=\" \".join(words)\n",
    "    result_corpus.append(text)\n",
    "  return result_corpus\n",
    "\n",
    "corpus=make_corpus(documents)\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', sublinear_tf=True, use_idf=True)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "svd = TruncatedSVD(n_components=7, n_iter=5, random_state=42)\n",
    "newX=svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98469024]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "A = newX[0]  #「私は秋田犬が大好き。秋田犬は私が大好き。」\n",
    "B = newX[1]  #「私は犬が少し苦手。」\n",
    "print(cosine_similarity(A.reshape(1,-1), B.reshape(1,-1)))  #reshapeで1行7列の行列に変換、-1を引数として適切な次元数にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17729441, -0.09072649, -0.20928661,  0.73785634, -0.5761692 ,\n",
       "       -0.09609122,  0.03714835])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C=newX[9]  #「ぶどうとメロンは必需品。」\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07283883]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(A.reshape(1,-1), C.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9847)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyTorchによるcos類似度の算出\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "A = torch.FloatTensor(newX[0])\n",
    "B = torch.FloatTensor(newX[1])\n",
    "F.cosine_similarity(A, B, dim=0)  #dim：何次元目でcos類似度を計算するか、今回は(x,y,z)というベクトルの場合のxで計算\n",
    "                                  #テンソル：３次元以上の配列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* word2vec：ニューラルネットワークを使って分散表現を作成する特定の技術と、その技術を使って作られた分散表現のこと\n",
    "     * 単語の言語コンテキストを再構築するように訓練された浅い2層ニューラルネットワークであり、大きなコーパスを受け取って１つのベクトル空間を生成する\n",
    "* 単語を密なベクトルとして表現可能\n",
    "* 加算構成性：ある単語の分散表現を他の単語の分散表現の和で表せる性質　（例 ： 王 － 男 ＋ 女 ＝ 女王）\n",
    "* CBOW：文脈が与えられたときにその中心にある単語を当てるアルゴリズム\n",
    "     * コンテキスト単語の順序は問わない\n",
    "* skip-gram：中心にある単語が与えられたときにその文脈を当てるアルゴリズム\n",
    "     * 現在の単語に近ければ近いほど、コンテキスト単語の重みを大きくする\n",
    "\n",
    "* Word2Vecのパラメータ\n",
    "     * Word2Vec(sentences, sg=1, size=100, window=5, min_count=1)\n",
    "     * sentences：文書\n",
    "     * sg：1ならskip-gram,0ならCBOWで学習\n",
    "     * size：何次元の分散表現を獲得するかを指定\n",
    "     * window：コンテクストとして認識する前後の単語数を指定\n",
    "     * min_count：指定の数以下の出現回数の単語は無視する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snaka\\OneDrive\\デスクトップ\\自然言語処理入門\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile=\"C:\\\\Users\\\\snaka\\\\Desktop\\\\自然言語処理入門\\\\data.txt\"\n",
    "wakatifile=\"C:\\\\Users\\\\snaka\\\\Desktop\\\\自然言語処理入門\\\\wakati.txt\"\n",
    "\n",
    "with open(wakatifile, 'w', encoding=\"utf-8\") as f2: \n",
    "\n",
    "  with open(datafile, 'r', encoding=\"utf-8\") as f1: \n",
    "    for line in f1:\n",
    "      for token in t.tokenize(line, wakati=True):\n",
    "        f2.write(token+\" \")\n",
    "      f2.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "sentences = word2vec.LineSentence(wakatifile)  #単語の数の配列を作る\n",
    "model = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.00743496e-03 -6.34719327e-04  7.73871969e-03 -8.90236627e-03\n",
      " -8.10148474e-03 -8.30794126e-03 -3.23709147e-03  7.42293196e-03\n",
      " -5.43841720e-03 -1.02270199e-02  8.78741872e-03 -5.49487164e-03\n",
      "  8.67560878e-03 -4.39395197e-03  5.19383606e-03  4.26840642e-03\n",
      "  3.37211415e-03 -3.85301909e-03  2.67454702e-03 -1.10655986e-02\n",
      " -1.93760032e-03 -1.93877437e-03  9.05259885e-03 -4.69297403e-03\n",
      " -7.33597996e-03  4.25468851e-03 -2.09917198e-03 -3.13549279e-03\n",
      "  6.33709691e-03  4.34396090e-03 -2.82557006e-03 -5.05995013e-05\n",
      "  1.07623767e-02  5.80946682e-03  6.19873824e-03  6.03737356e-03\n",
      "  2.51220819e-03 -1.71260757e-03 -7.36611849e-03 -1.30660587e-03\n",
      " -1.28824543e-03 -7.99243746e-04 -6.76895538e-03  8.01676139e-03\n",
      " -5.89640951e-03 -7.95082841e-03 -3.24018695e-03 -2.36023869e-03\n",
      " -7.35935429e-03  1.68108940e-03 -4.29904601e-03 -1.98503747e-03\n",
      " -7.69211212e-03  1.66085199e-03  3.34484014e-03 -3.46983550e-04\n",
      " -4.65169502e-03 -1.96695840e-03  6.78372057e-03  3.70097975e-03\n",
      " -9.47262440e-03 -3.62573517e-03  3.07747861e-03  2.54776515e-03\n",
      " -1.06672235e-02  6.59952639e-03 -8.56265705e-03 -6.20155036e-03\n",
      "  1.22878572e-03 -2.40414892e-03  3.21947993e-03  1.00538461e-02\n",
      " -2.12331233e-03  9.97660495e-03  3.27824592e-03  9.26063396e-03\n",
      "  5.92817180e-03 -8.99279397e-03 -4.08830354e-03  6.12253277e-03\n",
      "  3.72291170e-03  9.44904611e-03  6.00318611e-03  9.31412634e-03\n",
      " -1.01845730e-02  3.27752973e-03 -4.05783858e-03  3.67332017e-03\n",
      "  6.86224783e-03  5.73287113e-03  8.98541417e-03  1.33140420e-03\n",
      " -1.25207577e-03  5.54226432e-03 -3.61041655e-03  4.53152636e-04\n",
      "  9.80676059e-03 -5.60877332e-03  3.87422554e-03 -8.22040346e-03]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['犬'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load(cwd + '\\\\chive-1.2-mc90_gensim\\\\chive-1.2-mc90.kv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54135144"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('葡萄', 'メロン')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('巨峰', 0.7546089887619019),\n",
       " ('果実', 0.6553981304168701),\n",
       " ('シャルドネ', 0.6500457525253296),\n",
       " ('ピオーネ', 0.6491853594779968),\n",
       " ('萄', 0.6364786028862)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('葡萄', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('フランス', 0.7039362192153931),\n",
       " ('ヨーロッパ', 0.6617158055305481),\n",
       " ('ロッパ', 0.6234431266784668)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['日本','パリ'], negative=['東京'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('不可能', 0.5032184720039368),\n",
       " ('状況下', 0.4760775566101074),\n",
       " ('容易', 0.4758133590221405)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['未来','困難'], negative=['夢'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* doc2vec：任意の長さの文章を固定長のベクトルに変換する技術\n",
    "* 文章や文書の分散表現を獲得する\n",
    "* word2vec→単語の分散表現\n",
    "* PV-DM：文章のidと単語を複数個渡し、次に出てくる単語を予測するというタスクを解きながら文章の分散表現を獲得するアルゴリズム\n",
    "     * word2vecのCBOWに対応\n",
    "* PV-DBOW：文書が与えられたときに、その中に出てくる単語（語順を考慮しない）を予測するネットワークを学習して、文書ベクトルを獲得するアルゴリズム\n",
    "     * word2vecのskip-gramに対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "with open('c:\\\\Users\\\\snaka\\\\OneDrive\\\\デスクトップ\\\\自然言語処理入門\\\\wakati2.txt', encoding=\"utf-8\") as f:\n",
    "  docs = [TaggedDocument(words=data.split(), tags=[i]) for i, data in enumerate(f)]  #空白ごとに区切る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "model = Doc2Vec(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mymodel.model') #モデルの保存\n",
    "model=Doc2Vec.load('mymodel.model') #読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snaka\\AppData\\Local\\Temp\\ipykernel_16196\\2021273564.py:1: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  model.docvecs[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00553779, -0.00615797, -0.01012321,  0.00866437,  0.00346033,\n",
       "       -0.00031075, -0.0094957 , -0.00424911, -0.01039819,  0.00155471,\n",
       "        0.00277572,  0.00395738, -0.00417691, -0.00287537, -0.00284936,\n",
       "       -0.00927095,  0.00265095,  0.00909252, -0.0096799 , -0.00425436,\n",
       "       -0.00352167,  0.00240364, -0.00504206,  0.00239101,  0.00586166,\n",
       "       -0.00812463, -0.00920627, -0.01002444,  0.00451112, -0.00946636,\n",
       "        0.00663882,  0.00689971, -0.00588151, -0.00482819, -0.00115095,\n",
       "        0.00220524, -0.00160562, -0.00870074, -0.00377032,  0.00171615,\n",
       "       -0.00159121, -0.00745074,  0.00404443, -0.0088798 ,  0.00295637,\n",
       "       -0.00482227,  0.00038301, -0.00251194,  0.00563254, -0.00768922,\n",
       "       -0.00187831, -0.00086241, -0.00675723, -0.00711295, -0.00195158,\n",
       "        0.008772  , -0.00105768,  0.00341701, -0.00595282,  0.00893064,\n",
       "        0.00316198,  0.00935392,  0.00514747, -0.00396496,  0.00191563,\n",
       "       -0.00355881,  0.00564387,  0.00216027, -0.00310016, -0.00549992,\n",
       "       -0.00819591, -0.000481  , -0.00826337, -0.00897717, -0.00703892,\n",
       "        0.00175495, -0.00580578, -0.00743663,  0.00188646,  0.00186985,\n",
       "        0.0077971 ,  0.00481401, -0.00986456,  0.00032532,  0.00717679,\n",
       "        0.00229504,  0.00289514, -0.00510476,  0.0066075 ,  0.00213641,\n",
       "       -0.00709212,  0.00702919, -0.00971718, -0.00818659, -0.00406682,\n",
       "        0.01013197,  0.00281074, -0.00200451,  0.00899984,  0.00212927],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs[0]  #IDが0のベクトルを得る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snaka\\AppData\\Local\\Temp\\ipykernel_16196\\2309477780.py:1: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  model.docvecs.most_similar(0, topn=2)  #IDが0の文書に最も類似した文書がIDが30の文書\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(30, 0.2341257929801941), (40, 0.2320420742034912)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(0, topn=2)  #IDが0の文書に最も類似した文書がIDが30の文書"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0941845e-03,  1.6542207e-03, -2.2828518e-03, -2.7250140e-03,\n",
       "       -2.6255255e-03,  3.7480987e-04, -1.5203730e-03,  1.9013119e-03,\n",
       "       -9.5949741e-04,  3.4623079e-03,  3.7278903e-03,  3.9796829e-03,\n",
       "        4.6212226e-03, -5.9217640e-04,  1.8265432e-03, -5.0215744e-03,\n",
       "        3.6066663e-03, -3.8924334e-03, -1.3782302e-03, -2.2157456e-03,\n",
       "       -1.9296191e-03, -1.0536737e-03,  1.1909648e-03, -3.8748612e-03,\n",
       "       -8.0215465e-04, -1.4819851e-03, -5.2664131e-03, -3.5088959e-03,\n",
       "        2.8387350e-03, -2.0329726e-03, -2.5814935e-03,  3.0649755e-07,\n",
       "       -3.8580671e-03, -4.6454724e-03, -3.4133866e-03,  2.4967412e-03,\n",
       "       -3.5047743e-03, -4.9026976e-03, -4.5988583e-03, -2.0452722e-03,\n",
       "        4.7840454e-04, -3.3606091e-03,  5.6390767e-04,  2.1878502e-03,\n",
       "       -5.1187846e-04,  2.8443641e-03, -3.5668560e-03,  4.5152384e-04,\n",
       "       -4.3709381e-03, -4.0892352e-05, -9.1278652e-04,  2.0399103e-03,\n",
       "       -3.8291735e-03,  1.7619805e-03,  2.5473309e-03,  3.3912859e-03,\n",
       "        2.1900658e-03, -7.8755344e-04, -1.7132685e-03, -3.2038756e-03,\n",
       "        9.5547043e-04,  2.2958149e-03,  1.3963222e-03,  1.3832885e-03,\n",
       "        3.5716882e-03,  1.2426721e-03,  2.8370451e-03,  5.0323373e-03,\n",
       "        1.7324745e-04,  5.7170022e-04, -2.3230165e-03, -2.3356858e-03,\n",
       "        5.2255830e-03,  3.6630889e-03, -3.3490458e-03, -3.6456452e-03,\n",
       "        2.9867811e-03,  2.9565105e-03, -1.4493581e-03, -4.0919785e-03,\n",
       "       -5.0627156e-03,  1.5896372e-03, -1.9944729e-03,  5.3988257e-03,\n",
       "        2.7842616e-04,  1.1126624e-03,  6.4581528e-04, -3.8078066e-03,\n",
       "        1.0605972e-03, -5.8931072e-04, -2.5904337e-03,  2.8145709e-03,\n",
       "       -1.2574788e-03, -1.1395968e-03, -1.0630017e-03, -1.4779172e-03,\n",
       "        1.5217053e-03, -2.0304578e-03,  8.2650961e-04,  2.7641677e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdoc = ['私', 'は', '秋田', '犬', 'が', '大好き', '。']\n",
    "model.infer_vector(newdoc)  #newdocのベクトルを推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
